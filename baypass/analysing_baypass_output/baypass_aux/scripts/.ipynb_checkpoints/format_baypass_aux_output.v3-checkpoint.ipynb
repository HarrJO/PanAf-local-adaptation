{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# format_baypass_aux_output.v3\n",
    "\n",
    "#### Harrison Ostridge, 22/08/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to this script, BayPass must be run on aux mode. This must be done three times with different seeds. This script combines all this data into one large data frame to make subsequent analysis in R much faster and easier. It also calculates the median values across runs with different seeds and combines results from different subsets if applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy over results\n",
    "\n",
    "non-genic chr21: Note that I can just use the normal chr21 coverage file as the non-genic SNPs are just a subset of this dataset and inner join will remove the genic information. Note slight diffrence in that these files do not have the chr prefix on chromosome numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing modules\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing modules\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype\n",
    "from functools import reduce\n",
    "import time\n",
    "\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories to loop over\n",
    "env_datas=['f_over_sum_known_trees']\n",
    "subsps=['all', 'ce', 'w'] #['all', 'ce', 'w']\n",
    "data='exome' # 'exome' or 'chr21'\n",
    "flanks='1000' # '1000' (makes not difference if data='exome')\n",
    "random_reps=None # None\n",
    "omega_run='' # ''\n",
    "\n",
    "# Paths\n",
    "cov_index_dir=\"../../../running_baypass/baypass_env_input\"\n",
    "output_dir=\"../output/formatted_baypass_aux_output/\"\n",
    "baypass_core_out_dir=\"../../baypass_core/output/formatted_baypass_core_output\"\n",
    "\n",
    "if data == 'exome':\n",
    "    baypass_aux_out_dir=\"../../../running_baypass/exome/output\"\n",
    "    allele_freq_dir=\"../../../../allele_frequencies/exome/output/\"\n",
    "    prefix='f5'\n",
    "    flanks_suffix=''\n",
    "    extra_pop=''\n",
    "if data == 'chr21':\n",
    "    baypass_aux_out_dir=\"../../../running_baypass/chr21/output\"\n",
    "    allele_freq_dir=\"../../../../allele_frequencies/chr21/output/\"\n",
    "    prefix='chr21.f7'\n",
    "    extra_pop='_pop'# Annonyingly, chr21 cover files contain '_pop' due to an inconsitencey in the file naming\n",
    "    if flanks != None:\n",
    "        flanks_suffix=\".non-genic_\"+flanks+\"bp.flanks\"\n",
    "    else:\n",
    "        flanks_suffix=''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'flanks' only referes to chr21 data and determines how far from a gene a SNP must be to be in non-genic-chr21. The default is 1000bp.\n",
    "\n",
    "The 'random_reps' option is a legacy of an old method to generate a null distribution by randomising the environmental data and re-running BayPass multiple times.\n",
    "\n",
    "'omega_run' is from previous experiments with using different omegas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env data: f_over_sum_known_trees\n",
      " Subspecies: all\n",
      "  Run: real\n",
      "   Subset 1 of 1\n",
      "  Combine data across runs\n",
      "  Add additional information\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0efb75f325b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;31m#### Add coverage data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;31m##### Read in coverage data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         coverage=pd.read_csv(allele_freq_dir+\"/\"+prefix+file_subsp+\".pops_minInd.6or50pct_missing.pops.\"+miss_pops1+\"/\"+af_file_prefix+miss_pops1+extra_pop+\"_minMAC2_coverage\", \n\u001b[0m\u001b[1;32m    161\u001b[0m                              sep=\"\\t\")    \n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_string_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "if random_reps is not None:\n",
    "    print(\"Including results from \"+str(max(random_reps))+\" random habitat assignments\\n\")\n",
    "# Loop over each environmental dataset\n",
    "for env_data in env_datas:\n",
    "    print(\"Env data: \"+env_data)\n",
    "    ## Get covariable index file\n",
    "    if \"1cat\" in env_data:\n",
    "        cov_index=pd.DataFrame([[1, env_data]], columns=['COVARIABLE', 'COVARIABLE_name'])\n",
    "    else:\n",
    "        cov_index=pd.read_csv(cov_index_dir+\"/\"+env_data+\".txt\", \n",
    "                              sep=\"\\s+\", names=['COVARIABLE', 'COVARIABLE_name'])\n",
    "    ## Loop over each subspecies dataset\n",
    "    for subsp in subsps:\n",
    "        print(\" Subspecies: \"+subsp)\n",
    "        start_time = time.time()\n",
    "        ### Assign values for each susbpecies\n",
    "        if subsp == 'all':\n",
    "            file_subsp=''\n",
    "        else:\n",
    "            file_subsp='.'+subsp\n",
    "        if subsp == 'n':\n",
    "            miss_pops1=str(0)\n",
    "            if data == 'chr21':\n",
    "                miss_pops2=str(0.0)\n",
    "            else:\n",
    "                miss_pops2=miss_pops1\n",
    "        else:\n",
    "            miss_pops1=str(0.3)\n",
    "            miss_pops2=miss_pops1\n",
    "        ### Set correct suffix for files in allele frequency directory\n",
    "        if data == 'exome':\n",
    "            af_file_prefix=prefix+file_subsp+\".pops.all.chrs_missing.pops.\"\n",
    "        if data == 'chr21':\n",
    "            af_file_prefix=prefix+file_subsp+\".pops_chr21_missing.pops.\"\n",
    "        ### Prepare list to store dfs from all runs (ie real, RANDOM_rep1, RANDOM_rep2 etc)\n",
    "        betai_merge_all_runs=[]\n",
    "        pdelta_merge_all_runs=[]\n",
    "        #### Assign subset lists if applicable\n",
    "        if data=='exome' and env_data=='continuous_variables' and subsp in ['ce']:\n",
    "            subset_suffixs=['_subset1of2', '_subset2of2']\n",
    "        elif data=='exome' and env_data=='continuous_variables' and subsp in ['all']:\n",
    "            subset_suffixs=['_subset1of5', '_subset2of5', '_subset3of5', '_subset4of5', '_subset5of5']\n",
    "        elif data=='chr21' and env_data=='continuous_variables' and subsp in ['all']:\n",
    "            subset_suffixs=['_subset1of2', '_subset2of2']\n",
    "        elif data=='exome' and env_data=='paper_2' and subsp in ['all']:\n",
    "            subset_suffixs=['_subset1of4', '_subset2of4', '_subset3of4', '_subset4of4']\n",
    "        #elif data=='chr21' and env_data=='paper_2' and subsp in ['all']:\n",
    "        #    subset_suffixs=['_subset1of2', '_subset2of2']\n",
    "        else:\n",
    "            subset_suffixs=['']\n",
    "        ### Run for the REAL run and the RANDOM habitat asignment runs\n",
    "        if random_reps is None:\n",
    "            runs = ['']\n",
    "        else:\n",
    "            runs= ['']+['_RANDOM_rep'+str(i) for i in random_reps]\n",
    "        for run in runs:\n",
    "            #### Set varibales according to the run type\n",
    "            if 'RANDOM' in run:\n",
    "                subdir='RANDOM/'\n",
    "                run_label=run[1:]\n",
    "            else:\n",
    "                subdir=''\n",
    "                run_label=\"real\"\n",
    "            print(\"  Run: \"+run_label)\n",
    "            #### Prime output lists\n",
    "            betai_merge=[]\n",
    "            pdelta_merge=[]\n",
    "            #### Loop over subsets (if there are any)\n",
    "            for subset_suffix in subset_suffixs:\n",
    "                if subset_suffixs ==['']:\n",
    "                    print(\"   Subset 1 of 1\")\n",
    "                else: \n",
    "                    print(\"   Subset: \"+subset_suffix)\n",
    "                ##### Read allele counts file - just need the genomic positions\n",
    "                coords=pd.read_csv(allele_freq_dir+\"/\"+prefix+file_subsp+\".pops_minInd.6or50pct_missing.pops.\"+miss_pops1+\"/\"+af_file_prefix+miss_pops2+\"_pop.allele.counts_minMAC2\"+flanks_suffix+subset_suffix, \n",
    "                                   sep=\"\\t\", usecols=[\"chr\",\"pos\"])    \n",
    "                ###### Add MRK column\n",
    "                coords['MRK']=range(1, coords.shape[0]+1)\n",
    "                ##### Make chr column numeric if it isn't already\n",
    "                if is_string_dtype(coords['chr']):\n",
    "                    coords['chr'] = coords['chr'].str.replace('chr','')\n",
    "                    #coords=coords[~coords[\"chr\"].str.contains('X|Y')]\n",
    "                    coords['chr'] = pd.to_numeric(coords['chr'])\n",
    "                ##### Prepare output list\n",
    "                betai=[]\n",
    "                pdelta=[]\n",
    "                ##### Run for focal run and the two independant repeats with different seeds\n",
    "                for rep in ['focal', 'r1', 'r2']:\n",
    "                    ###### Assign the relevant parameters for each run\n",
    "                    if rep =='focal': \n",
    "                        seed_name=\"\"\n",
    "                        col_suffix=\"\"\n",
    "                        cols=[\"COVARIABLE\", \"MRK\", \"SD_Beta\", \"PIP\", \"M_Beta\", \"BF(dB)\"]\n",
    "                    if rep =='r1': \n",
    "                        seed_name=\"_seed.100\"\n",
    "                        col_suffix=\".r1\"\n",
    "                        cols=[\"COVARIABLE\", \"MRK\", \"M_Beta\", \"BF(dB)\"]\n",
    "                    if rep =='r2': \n",
    "                        seed_name=\"_seed.10k\"\n",
    "                        col_suffix=\".r2\"\n",
    "                        cols=[\"COVARIABLE\", \"MRK\", \"M_Beta\", \"BF(dB)\"]\n",
    "                    ###### Read in file\n",
    "                    betai_tmp=pd.read_csv(baypass_aux_out_dir+\"/\"+prefix+file_subsp+\".pops_minInd.6or50pct_missing.pops.\"+miss_pops1+\"_minMAC2\"+flanks_suffix+\"/auxi/\"+env_data+\"/\"+subdir+\"\"+prefix+file_subsp+\".pops_missing.pops.\"+miss_pops1+\"_minMAC2\"+flanks_suffix+omega_run+\"_\"+env_data+subset_suffix+run+seed_name+\"_summary_betai.out\", \n",
    "                                          sep=\"\\s+\", usecols=cols)\n",
    "                    pdelta_tmp=pd.read_csv(baypass_aux_out_dir+\"/\"+prefix+file_subsp+\".pops_minInd.6or50pct_missing.pops.\"+miss_pops1+\"_minMAC2\"+flanks_suffix+\"/auxi/\"+env_data+\"/\"+subdir+\"\"+prefix+file_subsp+\".pops_missing.pops.\"+miss_pops1+\"_minMAC2\"+flanks_suffix+omega_run+\"_\"+env_data+subset_suffix+run+seed_name+\"_summary_Pdelta.out\", \n",
    "                                           sep=\"\\s+\")\n",
    "                    ###### Add suffix to column names if applicable\n",
    "                    betai_tmp.columns = ['{}{}'.format(col, '' if col in [\"COVARIABLE\", \"MRK\"] else col_suffix) for col in betai_tmp.columns]\n",
    "                    pdelta_tmp.columns = ['{}{}'.format(col, '' if col in [\"COVARIABLE\"] else col_suffix) for col in pdelta_tmp.columns]\n",
    "                    ###### Add to list of output dfs\n",
    "                    betai.append(betai_tmp)\n",
    "                    pdelta.append(pdelta_tmp)\n",
    "                ##### Add genomic coordinates to focal run df \n",
    "                betai[0]=pd.merge(coords, betai[0], on=[\"MRK\"], how='right')\n",
    "                ##### Merge dfs\n",
    "                betai=reduce((lambda left, right: pd.merge(left,right,on=[\"COVARIABLE\", \"MRK\"], how='inner')), betai)\n",
    "                pdelta=reduce((lambda left, right: pd.merge(left,right,on=[\"COVARIABLE\"], how='inner')), pdelta)\n",
    "                ##### Median values\n",
    "                betai['M_Beta.median']=betai[['M_Beta', 'M_Beta.r1', 'M_Beta.r2']].median(axis=1)\n",
    "                betai['BF(dB).median']=betai[['BF(dB)', 'BF(dB).r1', 'BF(dB).r2']].median(axis=1)\n",
    "                pdelta['M_P.median']=pdelta[['M_P', 'M_P.r1', 'M_P.r2']].median(axis=1)\n",
    "                ##### Add to output list\n",
    "                betai_merge.append(betai)\n",
    "                pdelta_merge.append(pdelta)\n",
    "            \n",
    "            #### Combine subsets\n",
    "            if subset_suffixs !=['']:\n",
    "                print(\"   Combine subsets\")\n",
    "            betai_merge=pd.concat(betai_merge, axis=0)\n",
    "            pdelta_merge=pd.concat(pdelta_merge, axis=0)\n",
    "            #### Betai: Reorder\n",
    "            betai_merge=betai_merge.sort_values(['chr', 'pos'], ascending=True)\n",
    "            #### Betai: add run suffix to column names and add to list\n",
    "            betai_merge.columns = ['{}{}'.format(col, '' if col in [\"chr\", \"pos\", \"COVARIABLE\", \"MRK\"] else run) for col in betai_merge.columns]     \n",
    "            betai_merge_all_runs.append(betai_merge)\n",
    "            #### Pdelta: add additional column\n",
    "            pdelta_merge['Run']=run_label\n",
    "            pdelta_merge_all_runs.append(pdelta_merge)\n",
    "            \n",
    "            ### Format _covariate.std file\n",
    "            #### Just read in one example (doesnt matter which subset or seed run as the file is the same)\n",
    "            cov_file=pd.read_csv(baypass_aux_out_dir+\"/\"+prefix+file_subsp+\".pops_minInd.6or50pct_missing.pops.\"+miss_pops1+\"_minMAC2\"+flanks_suffix+\"/auxi/\"+env_data+\"/\"+subdir+\"\"+prefix+file_subsp+\".pops_missing.pops.\"+miss_pops1+\"_minMAC2\"+flanks_suffix+omega_run+\"_\"+env_data+subset_suffix+run+seed_name+\"_covariate.std\", \n",
    "                                 sep=\"\\s+\", header=None)\n",
    "            #### Write the file in the new location with references to seed and subset removed from the file name\n",
    "            cov_file.to_csv(output_dir+\"/\"+prefix+file_subsp+\".pops_missing.pops.\"+miss_pops1+\"_minMAC2_\"+env_data+run+\"_covariate.std\",\n",
    "                                     sep=\"\\t\", header=False, index=False)\n",
    "        \n",
    "        ### Combine data across runs\n",
    "        print(\"  Combine data across runs\")\n",
    "        betai_merge_all_runs=reduce((lambda left, right: pd.merge(left,right,on=[\"chr\", \"pos\", \"COVARIABLE\", \"MRK\"], how='inner')), \n",
    "                                    betai_merge_all_runs)\n",
    "        pdelta_merge_all_runs=pd.concat(pdelta_merge_all_runs, axis=0)\n",
    "        ### Add covariable names\n",
    "        print(\"  Add additional information\")\n",
    "        betai_merge_all_runs=pd.merge(betai_merge_all_runs, cov_index, on=[\"COVARIABLE\"], how='inner')\n",
    "        pdelta_merge_all_runs=pd.merge(pdelta_merge_all_runs, cov_index, on=[\"COVARIABLE\"], how='inner')\n",
    "        ### Betai\n",
    "        #### Add coverage data\n",
    "        ##### Read in coverage data\n",
    "        coverage=pd.read_csv(allele_freq_dir+\"/\"+prefix+file_subsp+\".pops_minInd.6or50pct_missing.pops.\"+miss_pops1+\"/\"+af_file_prefix+miss_pops1+extra_pop+\"_minMAC2_coverage\", \n",
    "                             sep=\"\\t\")    \n",
    "        if is_string_dtype(coverage['chr']):\n",
    "            coverage['chr'] = coverage['chr'].str.replace('chr','')\n",
    "            coverage=coverage[~coverage[\"chr\"].str.contains('X|Y')]\n",
    "            coverage['chr'] = pd.to_numeric(coverage['chr'])\n",
    "        coverage.fillna(0)\n",
    "        coverage['coverage'] = coverage.drop(['chr', 'pos'], axis=1).sum(axis=1)\n",
    "        coverage=coverage[['chr', 'pos','coverage']]\n",
    "        betai_merge_all_runs=pd.merge(betai_merge_all_runs, coverage, on=[\"chr\", \"pos\"], how='inner')\n",
    "        if data=='exome':\n",
    "            #### Add annotation and XtX data - NB: doing two merges is much much faster than one and then dropping duplicate columns \n",
    "            ##### Read in annotated BayPass core output \n",
    "            xtx_ann=pd.read_csv(baypass_core_out_dir+\"/\"+prefix+file_subsp+\".pops_missing.pops.\"+miss_pops1+\"_minMAC2_summary_pi_xtx.out_row.per.gtf.annot_5000bp.flanks\", \n",
    "                                sep=\"\\t\", usecols=[\"chr\",\"pos\",\"MRK\",\"XtXst.med\",\"gene\"])\n",
    "            ##### Add XtXst\n",
    "            betai_merge_all_runs_xtx=pd.merge(betai_merge_all_runs, xtx_ann[[\"chr\",\"pos\",\"XtXst.med\"]].drop_duplicates(), on=[\"chr\", \"pos\"], how='inner')\n",
    "            ##### Add gene annotation\n",
    "            betai_merge_all_runs_ann=pd.merge(betai_merge_all_runs, xtx_ann[[\"chr\",\"pos\",\"XtXst.med\", \"gene\"]], on=[\"chr\", \"pos\"], how='inner')\n",
    "            #### Write betai\n",
    "            print(\"  Write\")\n",
    "            betai_merge_all_runs_xtx.to_csv(output_dir+\"/\"+prefix+file_subsp+\".pops_missing.pops.\"+miss_pops1+\"_minMAC2\"+omega_run+\"_\"+env_data+\"_summary_betai.out_all_runs.gz\",\n",
    "                                        sep=\"\\t\", header=True, index=False, \n",
    "                                        compression=\"gzip\")\n",
    "            betai_merge_all_runs_ann.to_csv(output_dir+\"/\"+prefix+file_subsp+\".pops_missing.pops.\"+miss_pops1+\"_minMAC2\"+omega_run+\"_\"+env_data+\"_summary_betai.out_row.per.gtf.annot_5000bp.flanks_all_runs.gz\",\n",
    "                                            sep=\"\\t\", header=True, index=False, \n",
    "                                            compression=\"gzip\")\n",
    "        if data=='chr21':\n",
    "            betai_merge_all_runs.to_csv(output_dir+\"/\"+prefix+file_subsp+\".pops_missing.pops.\"+miss_pops1+\"_minMAC2\"+flanks_suffix+omega_run+\"_\"+env_data+\"_summary_betai.out_all_runs.gz\",\n",
    "                                            sep=\"\\t\", header=True, index=False, \n",
    "                                            compression=\"gzip\")\n",
    "            \n",
    "        ### Write Pdelta\n",
    "        pdelta_merge_all_runs.to_csv(output_dir+\"/\"+prefix+file_subsp+\".pops_missing.pops.\"+miss_pops1+\"_minMAC2\"+flanks_suffix+omega_run+\"_\"+env_data+\"_summary_Pdelta.out_all_runs\",\n",
    "                                     sep=\"\\t\", header=True, index=False)\n",
    "        \n",
    "        print(\"  Completed in %s seconds\" % round(time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
